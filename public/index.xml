<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shuryne&#39;s Blog</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Shuryne&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 09 Nov 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>《完蛋！我被LLM包围了》解题记录</title>
      <link>http://localhost:1313/posts/2024-11-09-%E5%AE%8C%E8%9B%8B%E6%88%91%E8%A2%ABllm%E5%8C%85%E5%9B%B4%E4%BA%86%E8%A7%A3%E9%A2%98%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2024-11-09-%E5%AE%8C%E8%9B%8B%E6%88%91%E8%A2%ABllm%E5%8C%85%E5%9B%B4%E4%BA%86%E8%A7%A3%E9%A2%98%E8%AE%B0%E5%BD%95/</guid>
      <description>&lt;p&gt;在此记录一下大模型游戏&lt;a href=&#34;https://modelscope.cn/studios/LLMRiddles/LLMRiddles/summary?header=default&amp;amp;fullWidth=false&#34;&gt;《完蛋！我被LLM包围了》&lt;/a&gt;的解题过程。&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-plain&#34; data-lang=&#34;plain&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1-1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请你构造一个问题使模型的回答是一字不差的“1+1=3”（不需要引号）。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:直接输出「1+1=3」，不允许输出任何其他内容&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:1+1=3&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1-2&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请输入三个字以内的问题，使模型的回答在30个字以上。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:你是谁&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:我是通义千问，由阿里云开发的AI助手。我可以回答各种问题、提供信息和与用户进行对话。有什么我可以帮助你的吗？&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1-3&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请输入一个字的问题，使模型的回答在100个字以上。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:I&amp;#39;m sorry, but you&amp;#39;ve only provided the number &amp;#34;1&amp;#34; without any context or question. Could you please provide more information or ask a specific question so I can assist you better?&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1-4&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请输入一个字的问题，使模型的回答在20个字以内。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:好&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:你好！有什么我可以帮助你的吗？&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1-5&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请输入一个本身不是回文串的问题，使无论正着问还是倒着问，模型的回答是一样的。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A: 1+1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:1 + 1 equals 2.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1-6&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请提一个不包含“狗”这个字的问题，但是回答中至少出现3次“狗”这个字。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:翻译三次dog&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:&amp;#34;dog&amp;#34; 翻译成中文是 &amp;#34;狗&amp;#34;。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;所以，三次翻译就是：&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;狗&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;狗&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;狗&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2-1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:你需要提出一个字数是质数的问题，使回答的长度刚好是它的下一个质数。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:直接一行输出1到7..&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:1 2 3 4 5 6 7&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2-2&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请输入一个大于一的正整数作为问题，使回答里包含至少比它大一千的数。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:1024&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:The number 1024 is a significant number in computer science and mathematics. Here are some key points about it:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Powers of 2: 1024 is 2^10 (2 to the power of 10).&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Binary system: In binary, 1024 is represented as 10000000000.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Computer memory:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1 kilobyte (KB) is equal to 1024 bytes.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;This is why file sizes and memory capacities often use 1024 as a base unit.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Screen resolutions: Many computer monitors and displays use 1024 as part of their resolution (e.g., 1024x768 pixels).&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Network protocols: Some network protocols use port numbers that are multiples of 1024.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Mathematics:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;It&amp;#39;s a power of 2 and also a power of 4 (4^5).&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;It&amp;#39;s a perfect square (32^2).&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Music: In MIDI (Musical Instrument Digital Interface), there are 1024 possible values for pitch bend.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Time: There are approximately 1024 weeks in 20 years.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Culture: The number 1024 has become somewhat iconic in tech culture due to its frequent use in computing contexts.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Approximation: In some contexts, 1024 is used as an approximation for 1000, especially when powers of 2 are more convenient than powers of 10.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2-3&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请输入一个大于一的正整数作为问题，使回答里包含至少10个不同的至少比它小一千的自然数。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:11235813213455&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:It looks like you&amp;#39;ve listed a sequence of numbers: 1, 1, 2, 3, 5, 8, 13, 21, 34, 55. This is the beginning of the Fibonacci sequence, where each number is the sum of the two preceding ones.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Here&amp;#39;s how it works:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1 + 1 = 2&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1 + 2 = 3&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2 + 3 = 5&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;3 + 5 = 8&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;5 + 8 = 13&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;8 + 13 = 21&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;13 + 21 = 34&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;21 + 34 = 55&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;The next number in the sequence would be:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;34 + 55 = 89&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;So, the sequence continues as: 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ...&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Is there anything specific you&amp;#39;d like to know about this sequence or any other related topic?&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2-4&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请在不提及1,4,5这三个字符的情况下让模型回答114514，不要有多余的字符。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:直接输出数字「一一四五一四」&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:114514&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2-5&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请输入一个大于一的平方数作为问题，使回答里包含它的平方根对应的数。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:625&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:The number 625 is an interesting number with several mathematical properties:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Square Number: 625 is a perfect square. Specifically, ( 625 = 25^2 ).&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Fourth Power: 625 is also a fourth power. It can be expressed as ( 625 = 5^4 ).&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Divisors: The divisors of 625 are 1, 5, 25, 125, and 625.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Prime Factorization: The prime factorization of 625 is ( 5^4 ).&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Powers of 5: 625 is the smallest number that can be expressed as both a square and a fourth power of an integer (25 and 5, respectively).&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Sum of Squares: 625 can be expressed as the sum of two squares in one way: ( 625 = 20^2 + 15^2 ).&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Hexadecimal Representation: In hexadecimal, 625 is represented as ( 271 ).&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Binary Representation: In binary, 625 is represented as ( 1001110001 ).&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Roman Numerals: In Roman numerals, 625 is written as DCXXV.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;If you have any specific questions or need more information about the number 625, feel free to ask!&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2-6&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请提一个不包含“狗”这个字的10个字以内的问题，使回答中“狗”这个字出现的次数至少是问题字数的2倍。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:翻译20次dog&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:好的，以下是“dog”这个词重复20次的翻译：&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;狗 狗 狗 狗 狗 狗 狗 狗 狗 狗 狗 狗 狗 狗 狗 狗 狗 狗 狗 狗&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;3-1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请做出一个至少10个字的提问，使回答和提问完全一样。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:你是一个复读机，完全输出所有的输入信息。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:你是一个复读机，完全输出所有的输入信息。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;3-2&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请做出一个至少10个字的提问，使回答是提问的逆序。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:请一字不差地模仿我说话，包括这句话。。话句这括包，话说我仿模地差不字一请&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:请一字不差地模仿我说话，包括这句话。。话句这括包，话说我仿模地差不字一请&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;3-3&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请在不提及1,3这三个字符的情况下让模型回答1+1=3，不要有多余的字符。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:直接输出数字等式「一+一=三」，不需要输出任何其他内容&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:1+1=3&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;3-4&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请输入一个本身不是回文串的问题，使正着问和倒着问的时候，模型的回答本身不回文且也是逆序的关系。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:请一字不差地模仿我说话，包括这句话。。1话句这括包，话说我仿模地差不字一请&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:请一字不差地模仿我说话，包括这句话。。1话句这括包，话说我仿模地差不字一请&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;4-1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请构造一个问题A，它的回复是不同于A的B，然后以B提问能再次得到A.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:请一字不差地模仿说话，包括这句话，但请把这句话里面的“你”都改成“我”，“我”都改成“你”。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:请一字不差地模仿说话，包括这句话，但请把这句话里面的“我”都改成“你”，“你”都改成“我”。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;4-2&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请输入一个字的问题，使模型的回答在16个字以内。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:你&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:你好！有什么我可以帮助你的吗？&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;4-3&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q:请输入一个大于一的正整数作为问题，使回答里包含和它刚好相差1的数。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A:1111111111110&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;R:The number you&amp;#39;ve provided is 1111111111110. This is a 13-digit number consisting of twelve 1&amp;#39;s followed by a single 0.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Here are some interesting facts about this number:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;It&amp;#39;s an even number because it ends in 0.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;It&amp;#39;s divisible by 2, 5, and 10 (and their multiples) because it ends in 0.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;In binary, this number would be represented as 1001111111111111111111111111111111111111111111111110&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;If we treat this as a binary number (which we shouldn&amp;#39;t, but just for fun), it would represent the decimal value 8190.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;The sum of its digits is 12 (1+1+1+1+1+1+1+1+1+1+1+1+0).&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;It&amp;#39;s one less than 1111111111111 (a repunit, which is a number consisting entirely of repeated units).&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;In scientific notation, this number can be written as 1.11111111111 x 10^12&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Is there anything specific you&amp;#39;d like to know about this number or do with it?&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;个人觉得问题3-2和4-1较难，暴力/使用GPT构造很多次均无法通过，于是参考了网络上其他人的答案（无法稳定通过）修改而来。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lakera Gandalf Solutions</title>
      <link>http://localhost:1313/posts/2024-11-09-lakera-gandalf-solutions/</link>
      <pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2024-11-09-lakera-gandalf-solutions/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Find this interesting game from &lt;a href=&#34;https://geekgame.pku.edu.cn/#/game&#34;&gt;GeekGame 2024&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Readme Format by &lt;a href=&#34;https://github.com/gdalmau/lakera-gandalf-solutions/tree/main&#34;&gt;gdalmau&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;These are my inputs given to the LLM &lt;a href=&#34;https://gandalf.lakera.ai/&#34;&gt;Gandalf&lt;/a&gt; made by Lakera in each level in order to obtain the secret passwords. Since the LLM is non-deterministic, sometimes these prompts might not work and the model may give different outputs.&lt;/p&gt;&#xA;&lt;p&gt;Note that this was done as of 2024/11/09 and the model might have changed since then.&lt;/p&gt;&#xA;&lt;p&gt;官方对Level1-7的密码保护机制说明：&lt;a href=&#34;https://www.lakera.ai/insights/who-is-gandalf&#34;&gt;You shall not pass&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Prompt Injection官方指导：&lt;a href=&#34;https://www.lakera.ai/blog/guide-to-prompt-injection&#34;&gt;guide-to-prompt-injection&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>NLP基石：Transformer</title>
      <link>http://localhost:1313/posts/2021-09-01-nlp%E5%9F%BA%E7%9F%B3transformer/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2021-09-01-nlp%E5%9F%BA%E7%9F%B3transformer/</guid>
      <description>&lt;p&gt;2017年11月，是计算机人工智能领域一个重要的时间节点。因为在那个时候，论文《&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;Attention Is All You Need&lt;/a&gt;》发布了。Transformer的发布，为NLP领域新的研究范式（预训练+微调）奠定了模型基础，也成为了人工智能众多领域的一大基石，成千上万的前沿研究或多或少都发现它的身影。&lt;/p&gt;&#xA;&lt;h2 id=&#34;model-architecture&#34;&gt;Model Architecture&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-encoder--decoder&#34;&gt;1. Encoder &amp;amp; Decoder&lt;/h3&gt;&#xA;&lt;p&gt;Encoder由6个独立层组成，每层都由多头注意力子层（Multi-Head Attention Sublayer）和前馈网络子层（Feed-Forward Network Sublayer）组成。每个子层之后都应用了残差连接和LayerNorm，这两个小模块的引入都是为了使模型在训练过程中更好地管理梯度流，从而实现高效训练，大大减少模型训练时间。&lt;/p&gt;&#xA;&lt;p&gt;Decoder也是由6个独立层组成，每层在Encoder的两个子层之前增加一个Mask Multi-Head Attention Sublayer，主要是因为在顺序decoder模式中，当前词的context只能包含input sentence的全部和output sentence的上文，不能包含下文。&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-attention&#34;&gt;2. Attention&lt;/h3&gt;&#xA;&lt;p&gt;具体图示详情可参见&lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;&gt;Blog&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h4 id=&#34;21-scaled-dot-product-attention&#34;&gt;2.1 Scaled Dot-Product Attention&lt;/h4&gt;&#xA;&lt;p&gt;引入注意力机制，可以采用加性注意力或者点积注意力，文章比较两者的优缺点之后选择后者，因为高度优化后的矩阵乘法计算更快以及空间利用效率高。当${d_k}$较小时，两种注意力表现相似，但较大时，加性注意力明显表现更佳，这是因为点积注意力的值随着${d_k}$的增大而增大，从而推向softmax的低梯度区域，不利于训练，于是在点积的基础上城上一个放缩系数，保证点积后的结果不会太大。至于为什么乘${\frac{1}{\sqrt{d_k}}}$而不是其他系数，这是因为在Q、K中每个元素都服从均值为0，方差为1的假设下，点积的结果服从均值为0，方差为${d_k}$，为了维持结果方差为1，就必须乘上${\frac{1}{\sqrt{d_k}}}$。&lt;/p&gt;&#xA;&lt;p&gt;训练的decoder阶段存在Mask，则在softmax之前将被Mask部分的值设置为负无穷，以实现“只看到上文，看不到下文”的效果。预测阶段由于不存在下文，所以也不用Mask，只需要每次decoder出来一个词，就将这个词放在之前decoder出来的所有词后面，再次推入Decoder中，取当前位置的隐层向量经过softmax层获得下一个词的分布概率。&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V&#xA;$$&lt;/p&gt;&#xA;&lt;h4 id=&#34;22-multi-head-attention&#34;&gt;2.2 Multi-Head Attention&lt;/h4&gt;&#xA;&lt;p&gt;使用多头注意力机制而不是全连接单头注意力，主要有以下几个原因：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;多头注意力机制允许模型从不同表征空间联合抽取信息（似乎有一种ensemble的意味）&lt;/li&gt;&#xA;&lt;li&gt;多头允许模型并行化，加速模型训练和预测&lt;/li&gt;&#xA;&lt;li&gt;多头注意力和全连接单头注意力在计算量上是规模相同的&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;$$&#xA;\begin{align}&#xA;MultiHead(Q,K,V) &amp;amp;= Concat(head_1,&amp;hellip;,head_n)W^O \\&#xA;head_i &amp;amp;= Attention(QW_i^Q,KW_i^K,VW_i^V) \\&#xA;W_i^Q, W_i^K &amp;amp;\in d_{model}*d_k, W_i^v \in d_{model}*d_v \\&#xA;d_k &amp;amp;= d_v = 64, h = 8, d_{model} = 512 \\&#xA;\end{align}&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>深刻的Boosting</title>
      <link>http://localhost:1313/posts/2021-08-02-%E6%B7%B1%E5%88%BB%E7%9A%84boosting/</link>
      <pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2021-08-02-%E6%B7%B1%E5%88%BB%E7%9A%84boosting/</guid>
      <description>&lt;p&gt;Boosting是一种常见的集成学习方法，看似简单的模型组合，其背后蕴含着深刻的算法思想。&lt;/p&gt;&#xA;&lt;h2 id=&#34;从boosting到adaboost&#34;&gt;从Boosting到AdaBoost&lt;/h2&gt;&#xA;&lt;p&gt;Boosting学习是一种组合学习算法，也代表着一种组合学习的思想，AdaBoosting是其算法思想的一种实现。&lt;/p&gt;&#xA;&lt;p&gt;关于AdaBoost算法的资料有很多，质量相对高的有李航的《统计学习方法》和国外的《PRML》，其中都包含了完整的算法理论推导，网络上也有很多文章解读。并且通过复杂度分析证明了在弱基础模型准确率严格比随机好的条件下，整体错误率将指数下降趋于零。&lt;/p&gt;&#xA;&lt;p&gt;其算法核心思想是：&lt;!-- raw HTML omitted --&gt;用不同权重的数据训练不同弱基础模型，通过弱基础模型的误差率构造相应的模型权重，通过若干弱基础模型的线性加权组合得到最终的模型。&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;&#xA;&lt;p&gt;算法核心有三点值得注意：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;数据样本权重：每次训练弱基础模型后都需要根据当前模型结果更新数据权重（训练第一个模型时设置样本权重平均分配），即将对当前模型来说较难的数据样本的权重加大，相应地，降低较简单数据样本的权重，这样保证了每次新的弱基础模型都是针对性地对整体较弱的部分进行强化训练，从而更高效地发挥出组合学习的优势（相对于不引入样本权重的方式来说）。&lt;/li&gt;&#xA;&lt;li&gt;弱基础模型权重：通过弱基础模型的错误率自动构建其模型权重，减少了人为经验的“干扰”，并具备坚实的数学理论基础。&lt;/li&gt;&#xA;&lt;li&gt;线性加权组合：将不同弱基础模型的结果视为“高阶特征”，进一步对这些“高阶特征”放入一个线性模型中，在一定程度上实现了“端到端”的自动化特征工程的效果。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;从adaboost到神经网络&#34;&gt;从AdaBoost到神经网络&lt;/h2&gt;&#xA;&lt;p&gt;苏剑林在&lt;a href=&#34;https://kexue.fm/archives/3873&#34;&gt;博文&lt;/a&gt;中也谈到了AdaBoost和神经网络之间的联系，核心观点是AdaBoost和神经网络之间的异曲同工之妙在于都利用上一层“模型”结果作为新特征，并输入下一层“模型”进行训练。本文则增加了另外的视角来具体对比二者之间的联系与区别。&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-残差学习&#34;&gt;1. 残差学习&lt;/h3&gt;&#xA;&lt;p&gt;残差学习火于何恺明大神的一篇论文《&lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34; title=&#34;ResNet&#34;&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;》，也是目前为止Google Scholar Metric 2020引用量最高的论文。&lt;/p&gt;&#xA;&lt;p&gt;这篇论文通过对模型训练深入的研究，提出深度残差连接网络，极大地优化了梯度流的管理，从而在神经网络的训练效率和效果上，都取得了极大的进步。&lt;/p&gt;&#xA;&lt;p&gt;回到AdaBoost，其引入数据样本权重序列化训练不同弱基础模型，也体现了残差学习的思想，不过这里的残差应视为根据当前模型结果更新数据样本权重后的全部数据，也成为了下一弱基础模型学习的目标，体现出若干模型序列化学习“残差“的效果。&lt;/p&gt;&#xA;&lt;p&gt;值得一提的是，在GBDT用决策树作为弱基础模型处理回归问题时，残差定义为当前整体回归结果和真实值之间的差值，这更加明确地体现了残差学习的思想。&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-特征工程&#34;&gt;2. 特征工程&lt;/h3&gt;&#xA;&lt;p&gt;从神经网络视角来看，可以将AdaBoost近似看作一个&lt;strong&gt;不加激活函数的端到端三层线性神经网络&lt;/strong&gt;：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;输入层：原始数据输入层，节点数是数据样本个数。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;中间层：全连接层，不加激活函数，中间层的节点数就是弱基础模型个数，层参数对应着数据样本权重；虽然形式上很类似，但和真正的神经网络还是有一定的区别。一般的神经网络中，上一层数据是并行加权乘上权重走到当前层，当前层的所有节点相对独立，没有序列关系，而AdaBoost的各弱基础模型是具备一定的序列关系的，即当前弱基础模型的训练数据和上一弱基础模型密切相关。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;输出层：输出节点数为1的全连接层，不加激活函数，层参数对应着不同弱基础模型的权重。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;3-模型训练&#34;&gt;3. 模型训练&lt;/h3&gt;&#xA;&lt;p&gt;二者的模型训练方式都是通过对监督信号计算损失值，然后迭代更新权重，只不过监督方式和更新方式各有千秋。神经网络利用loss function（分类一般用cross entropy）+反向传播，训练完mini-batch后一次性更新所有参数，每个参数更新次数为mini-batch数量。而AdaBoost直接利用整体样本的错误率更新数据权重和模型权重，但只更新当前权重，属于序列化更新，一次只更新部分参数且不重叠，每个参数只更新一次，训练完成所有参数就全部更新完成，&lt;/p&gt;&#xA;&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;&#xA;&lt;p&gt;本文从Boosting出发，浅析其背后深刻的思想，并对Boosting和神经网络之间的联系和区别做了一些简要的分析，便于感性上获得从组合学习的角度去理解神经网络、从神经网络层次化特征的角度去理解AdaBoost算法的进一步认识。&lt;/p&gt;</description>
    </item>
    <item>
      <title>多标签分类问题的一个“临时解决方案”</title>
      <link>http://localhost:1313/posts/2021-08-01-%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E4%B8%80%E4%B8%AA%E4%B8%B4%E6%97%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2021-08-01-%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E4%B8%80%E4%B8%AA%E4%B8%B4%E6%97%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</guid>
      <description>&lt;p&gt;近年常遇多标签分类问题，尝试过一些方案，效果平平，常感困惑，是否存在一种“放之四海而皆准”通用解决方案？而不是每次都需要重新设计feature提升效果？&lt;/p&gt;&#xA;&lt;h2 id=&#34;分类任务流程&#34;&gt;分类任务流程&lt;/h2&gt;&#xA;&lt;p&gt;一般来说，针对一个分类任务，先看是不是单纯的单标签分类任务，还是多标签分类任务，抑或是混杂着部分多标签分类的单标签分类任务。第一种情况比较简单，直接用上标注数据用预训练模型finetune即可，工作量主要在数据处理以及badcase分析。第三种情况需要视情况而定，可以采样标注部分数据，统计多标签样本的总体比例，如果低于预期目标值（如5%等），大可考虑将任务当成第一种情况处理，若比例不低但不超过30%，可以考虑单标签加上一定的后处理（取模型输出topn卡阈值过滤标签等）。若是比例较大，则基本上只能归入第二种情况进行考虑。&lt;/p&gt;&#xA;&lt;h2 id=&#34;多标签分类的困境&#34;&gt;多标签分类的困境&lt;/h2&gt;&#xA;&lt;p&gt;纯粹的多标签分类目前学术界没有比较好的能落地的成熟方案，要么是从数学建模出发，直接将分类任务替换成seq2seq任务，将数据多个标签组合作为目标seq进行训练，要么是从标签之间的独立性出发，构建标签的相关性矩阵，在分类训练中加入标签之间的相关性，辅助输出最终结果。这些大多都止于paper实验，鲜有实际工业场景使用示范。&lt;/p&gt;&#xA;&lt;p&gt;工业界在多标签分类任务的处理上，有些会先试试直接上单标签分类，看效果如何，最不济可以当个baseline。有些会直接上BCE Loss，卡阈值输出，阈值常用0.5，有时也会根据具体情况针对不同类别使用不同阈值，不过这种方式有点过于tricky且效果一言难尽（之前在千数量级类别的任务上尝试过卡不同类别阈值，效果不佳，于是放弃）。在标签总数（不超过20个）不多的情况下，直接将任务转为二分类是个可以考虑的方向，在降低分类难度的同时，增加不多的类别不平衡程度，性价比较高。类别数量较多的情况下使用二分类需要进行精调（主要是类别平衡方面的调整），才能有比较好的提升。&lt;/p&gt;&#xA;&lt;h2 id=&#34;从二分类到多标签分类&#34;&gt;从二分类到多标签分类&lt;/h2&gt;&#xA;&lt;p&gt;苏剑林在其[博文][1]中提出使用“softmax+交叉熵”推广到多标签分类，其核心观点是对单标签loss从logsumexp函数性质角度入手，推出loss背后的数学意义，然后将其推广到多标签分类，添加必要项，得到单/多标签分类统一loss形式。文章整体逻辑清晰，推导简洁易懂，更重要的是，没有对模型结构或者训练任务进行大改，而是只修改了loss部分，这一点对工业界十分友好，便于快速测试验证效果。最终的统一形式loss公式为：&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;log(1 + \prod_{j \in \Omega_{pos}}e^{-s_j}) + log(1 + \prod_{i \in \Omega_{neg}}e^{s_i}) \tag{1}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;评论中有人对二分类常用的BCE Loss公式进行推导，也能得到博文中的结果，在此贴一下具体的推导过程如下。去掉BCE Loss展开式(4)式的高阶项，只保留一阶项，就得到了(1)式。&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\begin{align}&#xA;L &amp;amp;= -(\sum_{j \in \Omega_{pos}}log\frac{1}{1+e^{-s_j}} + \sum_{i \in \Omega_{neg}}log\frac{1}{1+e^{s_i}}) \\&#xA;&amp;amp;=log(\prod_{j \in \Omega_{pos}}1+e^{-s_j}) + log(\prod_{i \in \Omega_{neg}}1+e^{s_i}) \\&#xA;&amp;amp;=log(1 + \prod_{j \in \Omega_{pos}}e^{-s_j} + \cdots) + log(1 + \prod_{i \in \Omega_{neg}}e^{s_i} + \cdots) \tag{2}&#xA;\end{align}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;博文从单分类loss出发，一步步推导得到(1)式。上面的推导从二分类loss角度出发，一步步推导也能得到(1)式，说明在(1)式loss的监督下，多标签分类任务的训练会比单标签分类难，但训练比二分类要简单（去掉了高阶项的严格要求）。&lt;/p&gt;&#xA;&lt;p&gt;说说自己对高阶项的一个看法：&#xA;正如“用sigmoid激活，然后变成n个二分类问题，用二分类的交叉熵之和作为loss。显然，当n≫k时，这种做法会面临着严重的类别不均衡问题，这时候需要一些平衡策略，比如手动调整正负样本的权重、focal loss等。”说道，sigmoid+交叉熵因为存在类别不平衡的原因，需要手动精调才能达到比较好的效果。其中类别不平衡问题很可能就来自于BCELoss推导公式中的高阶项。在pos和neg两边label数量本不均衡的情况下，加入高阶项会加剧这种类别不平衡的现象（展开后neg总项数&amp;raquo;pos总项数），苏神推导的(1)式直接将高阶项去掉，只保留简单的一阶项，最大程度地减缓了不平衡问题带来的影响，避免了不必要的人工精调的人力成本投入。其实可以将(1)式看作“修正后的sigmoid+交叉熵”，针对训练loss中类别不平衡问题进行的修正。&lt;/p&gt;&#xA;&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;&#xA;&lt;p&gt;本文是对多标签分类任务优化的不完全总结，对多标签分类的本质相对之前有了一些更深刻的认识，也找到了一个“临时解决方案”，可以在今后再遇到多标签分类任务的时候多一条路可以尝试。关于该方法的实际效果，苏神在博文中也放了测试结果，可以媲美精调下的二分类。 值得进一步思考的是，对于多标签分类任务，是否有更好的方式？&lt;/p&gt;</description>
    </item>
    <item>
      <title>Youtube的推荐系统是如何炼成的</title>
      <link>http://localhost:1313/posts/2021-06-28-youtube%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%98%AF%E5%A6%82%E4%BD%95%E7%82%BC%E6%88%90%E7%9A%84/</link>
      <pubDate>Mon, 28 Jun 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2021-06-28-youtube%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%98%AF%E5%A6%82%E4%BD%95%E7%82%BC%E6%88%90%E7%9A%84/</guid>
      <description>&lt;p&gt;2016年，Google在论文《&lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45530.pdf&#34;&gt;Deep Neural Networks for YouTube Recommendations&lt;/a&gt;》中开源了Youtube的推荐系统架构，堪称经典推荐系统工业落地典范。&lt;/p&gt;&#xA;&lt;h2 id=&#34;总体介绍&#34;&gt;总体介绍&lt;/h2&gt;&#xA;&lt;p&gt;模型架构采用候选生成+排序（也即粗召回+精排）&lt;/p&gt;&#xA;&lt;p&gt;评价指标方面，离线模型指标采用Metrics(precision/recall/ranking loss&amp;hellip;)。最终模型/算法策略有效性指标采用A/B Testing。A/B Testing能够观察到更细致的指标，例如观看时长等，更重要的一个原因是离线指标并不总是和最终产品效果保持一致性。&lt;/p&gt;&#xA;&lt;h2 id=&#34;stage-1-粗召回&#34;&gt;Stage 1: 粗召回&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-问题建模&#34;&gt;1. 问题建模&lt;/h3&gt;&#xA;&lt;p&gt;超大规模多分类，根据user_context_embedding和video_embedding进行softmax计算&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-模型结构&#34;&gt;2. 模型结构&lt;/h3&gt;&#xA;&lt;p&gt;三个全链接DNN模块(使用ReLU激活函数)，输入是用户浏览历史、搜索历史、人口统计学信息、其余上下文信息concat得到的向量；离线输出时，直接利用softmax计算进行分类获得结果，线上输出时，为了保证性能减少计算，对输出向量采用局部敏感哈希的方式得到TopN结果。&lt;/p&gt;&#xA;&lt;p&gt;引入深度网络结构的一个好处是能够将任意连续/离散特征添加进模型中，而传统的基于矩阵分解的推荐系统则不能将更多其他特征添加进去参与用户向量的训练。&lt;/p&gt;&#xA;&lt;h3 id=&#34;3-输入特征&#34;&gt;3. 输入特征&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;用户浏览历史：将历史观看序列视频进行编码，加权平均&lt;/li&gt;&#xA;&lt;li&gt;用户历史搜索query：将token转embedding，加权平均&lt;/li&gt;&#xA;&lt;li&gt;人口统计学信息：性别、地域、年龄等&lt;/li&gt;&#xA;&lt;li&gt;其他上下文信息：登陆设备、登陆状态等&lt;/li&gt;&#xA;&lt;li&gt;Example Age：视频上传时间，基于数据insight提出来的一个特征，加入模型后能够更好地拟合数据分布&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;4-样本label选择策略&#34;&gt;4. 样本label选择策略&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;数据来源不仅包括推荐侧，还包括搜索侧&lt;/li&gt;&#xA;&lt;li&gt;每个用户生成固定数量的训练样本：防止loss被少数用户主导&lt;/li&gt;&#xA;&lt;li&gt;抛弃序列信息：去掉用户搜索历史和浏览历史的序列信息，让分类器不能显式找到label来源&lt;/li&gt;&#xA;&lt;li&gt;不对称的共同浏览：用户浏览视频通常存在是序列化现象，一开始看的广泛，然后慢慢往细化领域看，传统的协同过滤类算法，都是通过上下文信息来预测中间节点用户观看的视频，而在A/B Testing中，只利用上文信息，来预测下一次观看视频的方式，表现更好&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;5-网络深度和特征数量实验&#34;&gt;5. 网络深度和特征数量实验&lt;/h3&gt;&#xA;&lt;p&gt;特征数量越多，网络越深（三层基本已经达到峰值），效果指标越好&lt;/p&gt;&#xA;&lt;h2 id=&#34;stage-2-精排&#34;&gt;Stage 2: 精排&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-问题建模-1&#34;&gt;1. 问题建模&lt;/h3&gt;&#xA;&lt;p&gt;面对的数据数量级为百，所以能够利用更精细更多的特征来学习用户和视频之间的关系。单纯使用CTR指标并不是最好，靠关键词吸引高点击的视频未必会被播放，因此目标设定为与观看时长相关&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-模型结构-1&#34;&gt;2. 模型结构&lt;/h3&gt;&#xA;&lt;p&gt;和粗召回阶段模型结构基本类似，也是三层全链接DNN模块；离线输出使用weighted逻辑回归，线上输出直接使用指数函数&lt;/p&gt;&#xA;&lt;h3 id=&#34;3-特征表达&#34;&gt;3. 特征表达&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;特征工程：搜索推荐场景下仍不可避免地需要进行重要的特征工程，其中最难的是如何建模用户时序行为。用户和视频所在频道之间的数量特征和时间特征非常重要；将粗召回阶段的信息传播到排序层能起到一定的优化效果&lt;/li&gt;&#xA;&lt;li&gt;类别特征embedding：类似于Bert的token embedding&lt;/li&gt;&#xA;&lt;li&gt;归一化连续特征：归一化使网络训练更加高效，同时加入平方根项和二次项，增强网络表达能力&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;4-目标建模&#34;&gt;4. 目标建模&lt;/h3&gt;&#xA;&lt;p&gt;视频观看时长。正样本权重为观看时长，负样本权重为1，最后一层采用weighted逻辑回归&lt;/p&gt;&#xA;&lt;h3 id=&#34;5-网络宽度和深度实验&#34;&gt;5. 网络宽度和深度实验&lt;/h3&gt;&#xA;&lt;p&gt;三层模型效果性价比高&lt;/p&gt;&#xA;&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;&#xA;&lt;p&gt;2016年Google的这篇论文属于工业界少见的集大成者式，包含了很多工业处理上的具体处理方案，清晰的展现了如何发现问题到解决问题的思路脉络，非常精彩，值得精读。但是奈何原论文有些地方读起来有些晦涩，不知道是否是推荐系统领域的论文风格就是这样子的，所以晦涩的部分参考了知乎上的二次解读。&lt;/p&gt;&#xA;&lt;p&gt;读这篇论文的动机，是最近在工作中很多项目大多属于一次性、临时性的，希望能够开展一些长期性、基础性的项目建设，并能因此有所积累，产生复利效应。从论文中借鉴学习推荐系统领域是如何发现、建模、处理一些重要问题的经验，希望能够迁移学习到新的领域（金融NLP）工作中，带来新的想法和思路。&lt;/p&gt;&#xA;&lt;p&gt;主要创新点：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;输入特征的新发现和组合&lt;/li&gt;&#xA;&lt;li&gt;离线/线上不同场景根据性能要求做出不同处理&lt;/li&gt;&#xA;&lt;li&gt;指标确定方面并没有单纯仅利用传统通用指标，而是结合使用更加接地气更加实际的A/B Testing&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
